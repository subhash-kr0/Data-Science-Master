{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "1. Explain the core components of the Hadoop ecosystem and their respective roles in processing and\n",
    "storing big data. Provide a brief overview of HDFS, MapReduce, and YARN.\n",
    "\n",
    "2. Discuss the Hadoop Distributed File System (HDFS) in detail. Explain how it stores and manages data in a\n",
    "distributed environment. Describe the key concepts of HDFS, such as NameNode, DataNode, and blocks, and\n",
    "how they contribute to data reliability and fault tolerance.\n",
    "\n",
    "3. Write a step-by-step explanation of how the MapReduce framework works. Use a real-world example to\n",
    "illustrate the Map and Reduce phases. Discuss the advantages and limitations of MapReduce for processing\n",
    "large datasets.\n",
    "\n",
    "4. Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications.\n",
    "Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN.\n",
    "\n",
    "5. Provide an overview of some popular components within the Hadoop ecosystem, such as HBase, Hive, Pig,\n",
    "and Spark. Describe the use cases and differences between these components. Choose one component and\n",
    "explain how it can be integrated into a Hadoop ecosystem for specific data processing tasks.\n",
    "\n",
    "6. Explain the key differences between Apache Spark and Hadoop MapReduce. How does Spark overcome\n",
    "some of the limitations of MapReduce for big data processing tasks?\n",
    "\n",
    "7. Write a Spark application in Scala or Python that reads a text file, counts the occurrences of each word,\n",
    "and returns the top 10 most frequent words. Explain the key components and steps involved in this\n",
    "application.\n",
    "\n",
    "8. Using Spark RDDs (Resilient Distributed Datasets), perform the following tasks on a dataset of your\n",
    "choice:\n",
    "a. Filter the data to select only rows that meet specific criteria.\n",
    "b. Map a transformation to modify a specific column in the dataset.\n",
    "c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average).\n",
    "\n",
    "9. Create a Spark DataFrame in Python or Scala by loading a dataset (e.g., CSV or JSON) and perform the\n",
    "following operations:\n",
    "a. Select specific columns from the DataFrame.\n",
    "b. Filter rows based on certain conditions.\n",
    "c. Group the data by a particular column and calculate aggregations (e.g., sum, average).\n",
    "d. Join two DataFrames based on a common key.\n",
    "\n",
    "\n",
    "\n",
    "10. Set up a Spark Streaming application to process real-time data from a source (e.g., Apache Kafka or a\n",
    "simulated data source). The application should:\n",
    "a. Ingest data in micro-batches.\n",
    "b. Apply a transformation to the streaming data (e.g., filtering, aggregation).\n",
    "c. Output the processed data to a sink (e.g., write to a file, a database, or display it).\n",
    "\n",
    "11. Explain the fundamental concepts of Apache Kafka. What is it, and what problems does it aim to solve in\n",
    "the context of big data and real-time data processing?\n",
    "\n",
    "12. Describe the architecture of Kafka, including its key components such as Producers, Topics, Brokers,\n",
    "Consumers, and ZooKeeper. How do these components work together in a Kafka cluster to achieve data\n",
    "streaming?\n",
    "\n",
    "13. Create a step-by-step guide on how to produce data to a Kafka topic using a programming language of\n",
    "your choice and then consume that data from the topic. Explain the role of Kafka producers and consumers\n",
    "in this process.\n",
    "\n",
    "14. Discuss the importance of data retention and data partitioning in Kafka. How can these features be\n",
    "configured, and what are the implications for data storage and processing?\n",
    "\n",
    "15. Give examples of real-world use cases where Apache Kafka is employed. Discuss why Kafka is the\n",
    "preferred choice in those scenarios, and what benefits it brings to the table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Ecosystem and Core Components \n",
    "\n",
    "```markdown\n",
    "\n",
    "\n",
    "\n",
    "1. Core Components of the Hadoop Ecosystem\n",
    "The Hadoop ecosystem is a suite of tools and frameworks for processing and storing large datasets in a distributed computing environment. The core components include:\n",
    "\n",
    "HDFS (Hadoop Distributed File System): A distributed file system that stores data across multiple machines.\n",
    "MapReduce: A programming model for processing large datasets with a parallel, distributed algorithm.\n",
    "YARN (Yet Another Resource Negotiator): A resource management layer that schedules and manages computational resources in the cluster.\n",
    "\n",
    "2. Hadoop Distributed File System (HDFS)\n",
    "HDFS is designed to store large datasets reliably and to stream those datasets at high bandwidth to user applications. It is highly fault-tolerant and designed to be deployed on low-cost hardware. Key concepts include:\n",
    "\n",
    "NameNode: The master server that manages the file system namespace and controls access to files by clients.\n",
    "DataNode: Nodes where actual data resides. Data is divided into blocks and distributed across the DataNodes.\n",
    "Blocks: The smallest unit of data storage in HDFS, typically 128 MB in size. Data is split into blocks and stored on different DataNodes.\n",
    "Data Reliability and Fault Tolerance:\n",
    "\n",
    "Replication: Each block is replicated across multiple DataNodes to ensure fault tolerance.\n",
    "Heartbeat and Block Reports: DataNodes periodically send heartbeats and block reports to the NameNode to confirm their status and the blocks they hold.\n",
    "Data Integrity: HDFS uses checksums to verify data integrity.\n",
    "\n",
    "3. MapReduce Framework\n",
    "MapReduce is a programming model used for processing and generating large datasets. It divides the task into two main phases:\n",
    "\n",
    "Map Phase:\n",
    "\n",
    "Input data is split into independent chunks.\n",
    "The Map function processes each chunk and generates key-value pairs.\n",
    "Reduce Phase:\n",
    "\n",
    "All key-value pairs from the Map phase are shuffled and sorted by key.\n",
    "The Reduce function processes each group of key-value pairs and aggregates the results.\n",
    "Real-World Example: Word Count\n",
    "\n",
    "Map Phase: Read a text file, split it into words, and emit each word with a count of 1.\n",
    "Reduce Phase: Aggregate the counts for each word.\n",
    "Advantages:\n",
    "\n",
    "Scalability: Processes petabytes of data across many machines.\n",
    "Fault Tolerance: Handles node failures gracefully.\n",
    "Limitations:\n",
    "\n",
    "Latency: Not suitable for real-time processing.\n",
    "Complexity: Requires writing complex code for simple tasks.\n",
    "\n",
    "4. Role of YARN in Hadoop\n",
    "YARN is the resource management layer of Hadoop. It allows multiple data processing engines to handle data stored in a single platform, providing:\n",
    "\n",
    "Resource Management: Allocates resources to various applications running in a cluster.\n",
    "Job Scheduling: Manages the scheduling of tasks.\n",
    "Comparison with Hadoop 1.x:\n",
    "\n",
    "Hadoop 1.x: Resource management and job scheduling were handled by JobTracker, leading to scalability issues.\n",
    "YARN: Separates resource management and job scheduling, improving scalability and resource utilization.\n",
    "Benefits of YARN:\n",
    "\n",
    "Scalability: Supports thousands of nodes and applications.\n",
    "Flexibility: Allows various processing models like real-time streaming and batch processing.\n",
    "\n",
    "5. Popular Components within the Hadoop Ecosystem\n",
    "HBase: A distributed, scalable, NoSQL database.\n",
    "Hive: A data warehouse infrastructure that provides data summarization and ad-hoc querying.\n",
    "Pig: A high-level platform for creating MapReduce programs using a scripting language called Pig Latin.\n",
    "Spark: A fast and general-purpose cluster computing system for big data.\n",
    "Use Case: Hive for Data Warehousing\n",
    "\n",
    "Hive can be integrated into a Hadoop ecosystem to perform data warehousing tasks. It allows SQL-like querying on large datasets stored in HDFS, enabling easy data analysis and reporting.\n",
    "\n",
    "6. Key Differences between Apache Spark and Hadoop MapReduce\n",
    "Speed: Spark performs in-memory processing, making it faster than MapReduce, which writes intermediate results to disk.\n",
    "Ease of Use: Spark has APIs for Java, Scala, Python, and R, making it more accessible.\n",
    "Advanced Analytics: Spark supports advanced analytics, including machine learning and graph processing.\n",
    "Overcoming Limitations:\n",
    "\n",
    "Spark's in-memory processing reduces I/O operations, addressing the latency issue of MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "\n",
    "7. Spark Application: Word Count\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "# Read the text file\n",
    "text_file = sc.textFile(\"path/to/textfile.txt\")\n",
    "\n",
    "# Perform word count\n",
    "word_counts = (text_file.flatMap(lambda line: line.split(\" \"))\n",
    "                         .map(lambda word: (word, 1))\n",
    "                         .reduceByKey(lambda a, b: a + b))\n",
    "\n",
    "# Get top 10 most frequent words\n",
    "top_words = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "# Print the results\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "\n",
    "8. Spark RDD Operations\n",
    "\n",
    "# Assume 'rdd' is an existing RDD\n",
    "\n",
    "# a. Filter\n",
    "filtered_rdd = rdd.filter(lambda x: x['age'] > 30)\n",
    "\n",
    "# b. Map transformation\n",
    "mapped_rdd = rdd.map(lambda x: (x['name'], x['age'] * 2))\n",
    "\n",
    "# c. Reduce aggregation\n",
    "sum_age = rdd.map(lambda x: x['age']).reduce(lambda a, b: a + b)\n",
    "\n",
    "\n",
    "9. Spark DataFrame Operations\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "df = spark.read.csv(\"path/to/dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# a. Select specific columns\n",
    "selected_df = df.select(\"column1\", \"column2\")\n",
    "\n",
    "# b. Filter rows\n",
    "filtered_df = df.filter(df[\"column1\"] > 50)\n",
    "\n",
    "# c. Group and aggregate\n",
    "grouped_df = df.groupBy(\"column1\").agg({\"column2\": \"sum\", \"column3\": \"avg\"})\n",
    "\n",
    "# d. Join DataFrames\n",
    "df1 = spark.read.csv(\"path/to/dataset1.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"path/to/dataset2.csv\", header=True, inferSchema=True)\n",
    "joined_df = df1.join(df2, df1[\"id\"] == df2[\"id\"])\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "10. Spark Streaming Application\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StructuredNetworkWordCount\").getOrCreate()\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "\n",
    "11. Apache Kafka\n",
    "Kafka is a distributed streaming platform that:\n",
    "\n",
    "Publishes and subscribes to streams of records.\n",
    "Stores streams of records in a fault-tolerant way.\n",
    "Processes streams of records as they occur.\n",
    "Problems Solved:\n",
    "\n",
    "High-throughput messaging.\n",
    "Real-time data processing.\n",
    "Data integration.\n",
    "\n",
    "12. Kafka Architecture\n",
    "\n",
    "Producers: Publish data to Kafka topics.\n",
    "Topics: Categories to which records are sent.\n",
    "Brokers: Kafka servers that store data and serve clients.\n",
    "\n",
    "Consumers: Read data from topics.\n",
    "ZooKeeper: Manages the Kafka cluster.\n",
    "\n",
    "13. Kafka Data Production and Consumption\n",
    "\n",
    "Producer:\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "producer.send('my-topic', b'some_message_bytes')\n",
    "producer.close()\n",
    "\n",
    "Consumer:\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer('my-topic', bootstrap_servers='localhost:9092')\n",
    "for message in consumer:\n",
    "    print(message.value)\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "\n",
    "14. Kafka Data Retention and Partitioning\n",
    "Data Retention: Configured via log.retention.hours, controlling how long records are stored.\n",
    "Partitioning: Distributes data across multiple brokers, configured by specifying the number of partitions.\n",
    "Implications:\n",
    "\n",
    "Improved scalability and load balancing.\n",
    "Configurable data longevity.\n",
    "\n",
    "\n",
    "15. Real-World Use Cases of Kafka\n",
    "Examples:\n",
    "\n",
    "Log Aggregation: Centralizing logs from different systems.\n",
    "Stream Processing: Real-time analytics and monitoring.\n",
    "Event Sourcing: Building event-driven architectures.\n",
    "Benefits:\n",
    "\n",
    "High throughput and fault tolerance.\n",
    "Scalability and durability.\n",
    "By completing these tasks, you will gain a comprehensive understanding of Hadoop, Spark, and Kafka, their roles in big data processing, and how to implement various data processing tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
