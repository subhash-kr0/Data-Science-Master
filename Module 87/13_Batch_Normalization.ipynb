{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kv-dHhBMBub"
      },
      "source": [
        "#Deep Learning With Computer Vision And Advanced NLP (DL_CV_NLP)\n",
        "\n",
        "$$ Revision Notes $$\n",
        "$$ A-Note-by-**Bappy Ahmed** $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejLy9iyPMFN9"
      },
      "source": [
        "# Batch Normalization:\n",
        "\n",
        "For training faster and getting good accuracy we have used till now,\n",
        "  - Activation functions (helps to avoid vanishing & exploding gradient problem)\n",
        "  - Transfer Learning\n",
        "  - Weight Initialization techniques\n",
        "\n",
        "But still after above solution you will get vanishing and exploding gradient issue, Maybe some condition is there. As per our observation till now,\n",
        "\n",
        "   <img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/17.png?raw=true\" width=\"600\"\n",
        "     height=\"300\">\n",
        "\n",
        "**Note**: If we are this slope region then we can avoid this issue.\n",
        "\n",
        " - ### Internal Covariate Shift (ICS):\n",
        "\n",
        "  Change in distribution of Network activation\n",
        "  >>>>Due to\n",
        "\n",
        "  Change in network parameter during training\n",
        "\n",
        "  **Note**: If we reduce ICS effect then it will give improved training speed.\n",
        "\n",
        "  - It has been observed & exprimented proven also by (Lecun et al. in 1998 & wisler and Navy 2011).\n",
        "  The network converge faster if inputs are whitened   (linearly transform to 0 mean and unit variance)\n",
        "\n",
        "  - Expectation is fix distribution for each layer to reduce ICS problem and get faster training.\n",
        "\n",
        "\n",
        "So, this expectation was solved by Batch Normalization (BN)\n",
        "\n",
        "## Now lets see about Batch Normalization (BN):\n",
        "In 2015  Serge and Christian Szegedy they introduced this Batch Normalization (BN).\n",
        "\n",
        "Batch Normalization (BN) is nothing but it's a set of operation added brfore or after on each activation layer for centering the data in that slope range of activation functions.\n",
        "\n",
        "#### Batch Normalization Algorithms and steps:\n",
        "  1. Calculate the mean of the batches (mini batch mean), $$\\mu_B = \\frac{1}{m_B} \\sum_{i=1}^{m_B}x^{(i)} $$\n",
        "\n",
        "  2. Calculate the varience of mini batch,\n",
        "  $$\\sigma_{B}^2 = \\frac{1}{m_B} \\sum_{i=1}^{m_B} (x^{(i)} - \\mu_B)^2  $$\n",
        "\n",
        "  3. Now normalize the inputs,\n",
        "  $$\\hat{x}^{(i)} = \\frac{x^{(i)}- \\mu_B}{\\sqrt \\sigma_B^2 + \\varepsilon }$$\n",
        "\n",
        "  Here, $\\varepsilon$ is smoothing term to avoid 0 division error.\n",
        "\n",
        "  4. Finally Calculate $z^{(i)} = \\gamma \\otimes \\hat{x}^{(i)} + \\beta$\n",
        "\n",
        "  Here, $\\gamma$ for scaling inputs & $\\beta$ for shifting inputs and these are learnable parameters.\n",
        "\n",
        "**Note**: If you use Batch Normalization then your model's parameters will be increased by $\\gamma$ and $\\beta$. Initially it would be defined randomly.\n",
        "\n",
        "\n",
        "\n",
        "   <img src=\"https://image.slidesharecdn.com/batchnormalization-181218143156/95/batch-normalization-11-638.jpg?cb=1545143985\" width=\"600\"\n",
        "     height=\"300\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPltdZaZLt0o"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}