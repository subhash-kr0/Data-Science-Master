{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ8OfSc8zLQs"
      },
      "source": [
        "#Deep Learning With Computer Vision And Advanced NLP (DL_CV_NLP)\n",
        "\n",
        "$$ Revision Notes $$\n",
        "$$ A-Note-by-**Bappy Ahmed** $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwqs8bDhzNoc"
      },
      "source": [
        "# Different optimizers:\n",
        "\n",
        "Optimizers are algorithm or methods used to change the attributes of the neural network such as weights & biases to reduce the losses. Optimizers are used to solve optimization problem by minimizing the function.\n",
        "\n",
        "   <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200511223856/f110.jpg\" width=\"600\"\n",
        "     height=\"300\">\n",
        "\n",
        "\n",
        "## Different optimizers:\n",
        " - Gradient Descent (GD)\n",
        " - Stochastic Gradient Descent (SGD)\n",
        " - Momentum based GD\n",
        " - Nesterov Accelerated Gradient (NAG)\n",
        " - Adagrad (Adaptive Gradinent)\n",
        " - RMSprop\n",
        " - Adam\n",
        "  - Adamax\n",
        "  - Nadam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye5yqf7R1gXb"
      },
      "source": [
        "#Drawback of Gradient Descent:\n",
        "Observing...\n",
        "\n",
        "$w_{new} = w_{old} - \\eta \\frac{\\partial c}{\\partial w}|_{w=w_{old}}$\n",
        "\n",
        "\n",
        "- **1st step**:\n",
        "\n",
        "  initial weight = $w_0$\n",
        "\n",
        "  $\\therefore w_1 = w_0 - \\eta \\frac{\\partial c}{\\partial w_0}....(1)$\n",
        "\n",
        "\n",
        "- **2nd step**:\n",
        "\n",
        "  weight = $w_1$\n",
        "\n",
        "  $\\therefore w_2 = w_1 - \\eta \\frac{\\partial c}{\\partial w_1}....(2)$\n",
        "\n",
        "So here, $(1)>(2)$\n",
        "\n",
        "### **Observation**:\n",
        " - $\\bigtriangledown w \\uparrow if \\frac{\\partial c}{\\partial w} \\uparrow $ more steep (Huge update)\n",
        "\n",
        " - $\\bigtriangledown w \\downarrow if \\frac{\\partial c}{\\partial w} \\downarrow $ less steep (less update)\n",
        "\n",
        " - It will be stuck at saddle point\n",
        " - It takes more time to convergence\n",
        " - It doesn't depend upon past weight update or accumulation\n",
        " - It is very slow for deep NN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmNJ94vm62NE"
      },
      "source": [
        "# Momentum based GD\n",
        "\n",
        "It was proposed by Boris Polyak in 1964. The idea was when a ball is rolling down a hill it starts very slow but picks up momentum and reaches terminal velocity. This idea comes from Physics. In physics,\n",
        "$$momentum = mass*velocity$$\n",
        "$$p= m.v$$\n",
        "\n",
        "- terminal velocity => Maximum attainable velocity during a free fall in a fluid or air.\n",
        "\n",
        "It reaches terminal velocity by accumulating past change in velocity. Similarly in our algoritgm. Now gradient will be used to accelerated gradinet descent. We introduce here a new term is momentum (m),\n",
        "\n",
        "$m \\leftarrow \\beta m+ \\eta \\bigtriangledown_\\theta J(\\theta)$\n",
        "\n",
        "$\\theta \\leftarrow \\theta - m$\n",
        "\n",
        "or\n",
        "\n",
        "$\\theta \\leftarrow \\theta - \\beta m + \\eta \\bigtriangledown_\\theta J(\\theta)$\n",
        "\n",
        "$m= \\beta m +\\eta \\frac{\\partial c}{\\partial w}|_{w=w}$   [Assumption bias = 0]\n",
        "\n",
        "$\\therefore w= w-m$\n",
        "\n",
        "Here $\\beta $ is coefficient of momentum term or friction. we keep $\\beta = 0.9$ it is exprimentally proven. But we can tweak also.\n",
        "\n",
        "- ### Observation:\n",
        " ### **step 1**:-\n",
        "\n",
        " initially, $w= w_0, m_0=0, \\beta= 0.9$\n",
        "\n",
        " so,\n",
        "\n",
        " $m_1 = \\beta m_0 + \\eta \\frac{\\partial c}{\\partial w}|_{w=w_0}$\n",
        "\n",
        " =>$m_1 =\\eta \\frac{\\partial c}{\\partial w}|_{w=w_0}$\n",
        "\n",
        " $w_1 = w_0 -m_1$\n",
        "\n",
        " $\\therefore w_1= w_0 -\\eta \\frac{\\partial c}{\\partial w}|_{w=w_0} \\rightarrow$ similar to GD\n",
        "\n",
        "\n",
        " ### **step 2:-**\n",
        "\n",
        " $m_1 =\\eta \\frac{\\partial c}{\\partial w}|_{w=w_0}, \\beta = 0.9, w= w_1$\n",
        "\n",
        " so,\n",
        "\n",
        " $m_2 = \\beta m_1 + \\eta \\frac{\\partial c}{\\partial w}|_{w=w_1}$\n",
        "\n",
        " => $m_2 = \\beta \\eta \\frac{\\partial c}{\\partial w}|_{w=w_0} + \\eta \\frac{\\partial c}{\\partial w}|_{w=w_1}$\n",
        "\n",
        " =>$m_2 = \\eta [\\beta \\frac{\\partial c}{\\partial w}|_{w=w_0} + \\frac{\\partial c}{\\partial w}|_{w=w_1}]$\n",
        "\n",
        " $\\therefore w_2 = w_1 - m_2$\n",
        "\n",
        " This time weight update will be larger.\n",
        "\n",
        " $if \\beta = 0$ then it just gradient descent.\n",
        "\n",
        " ### **step 3**:-\n",
        "\n",
        " $w = w_2, \\beta = 0.9, m_2 = \\eta [\\beta \\frac{\\partial c}{\\partial w}|_{w=w_0} + \\frac{\\partial c}{\\partial w}|_{w=w_1}]$\n",
        "\n",
        " so,\n",
        "\n",
        " $m_3 = \\beta m_2 + \\eta \\frac{\\partial c}{\\partial w}|_{w=w_2}$\n",
        "\n",
        " => $m_3 = \\beta \\eta [\\beta \\frac{\\partial c}{\\partial w}|_{w=w_0} + \\frac{\\partial c}{\\partial w}|_{w=w_1}] + \\eta \\frac{\\partial c}{\\partial w}|_{w=w_2}$\n",
        "\n",
        " => $m_3 = \\eta [\\beta^2 \\frac{\\partial c}{\\partial w}|_{w=w_0} + \\beta \\frac{\\partial c}{\\partial w}|_{w=w_1} + \\eta \\frac{\\partial c}{\\partial w}|_{w=w_2}]$\n",
        "\n",
        " $\\therefore w_3 = w_2 - m_3$\n",
        "\n",
        " This time weight update will be more than $w_2$\n",
        "\n",
        "\n",
        "## Drawbacks of Momentum:\n",
        "- One extra parameter $\\beta$ although $\\beta = 0.9$ works fine most of the cases.\n",
        "- It oscillates when it reaches closer to local minima or global minima, cz of the accumulation of past momentum. Sometimes it gets overshoot.\n",
        "\n",
        "## Advantages:\n",
        "- Momentum helps in fast convergence\n",
        "- oscillation can also help to come out local minima.\n",
        "\n",
        "## Keras implementation:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4OLNcxNyUFe"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.optimizers.SGD(learning_rate=0.01, momentum= 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6TArNUgNAmR"
      },
      "source": [
        "# Nesterov Accelerated Gradient (NAG):\n",
        "\n",
        "It was introduced by Yurii Nesterov in 1983. This is also known as Nesterov momentum optimization. It is faster that momentm optimmization.\n",
        "\n",
        "### Whats new?\n",
        "It calculates gradient slightly ahead in the direction of momentum. $(\\theta - \\beta m) or (w- \\beta m)$\n",
        "\n",
        "###Algorithm:\n",
        "\n",
        "$m \\leftarrow \\beta m + \\eta \\bigtriangledown_\\theta(\\theta-\\beta m)$\n",
        "\n",
        "$\\theta \\leftarrow \\theta - m$\n",
        "\n",
        "$m = \\beta m + \\eta \\frac{\\partial c}{\\partial w}|_{w= (w-\\beta m)}$\n",
        "\n",
        "$w= w-m$  [Assuming bias = 0]\n",
        "\n",
        "\n",
        "###step 1:-\n",
        "\n",
        "initially, $m_0 = 0, w= w_0, \\beta = 0.9$\n",
        "\n",
        "so,\n",
        "\n",
        "$m_1 = \\beta m_0 + \\eta \\frac{\\partial c}{\\partial w}|_{w= (w-\\beta m_0)} $\n",
        "\n",
        "=> $m_1 =  \\eta \\frac{\\partial c}{\\partial w}|_{w= w_0} \\rightarrow$ simple GD\n",
        "\n",
        "$\\therefore w_1 = w_0 - m_1$\n",
        "\n",
        "\n",
        "###step 2:-\n",
        "\n",
        " $m_1 = \\eta \\frac{\\partial c}{\\partial w}|_{w= w_0}, w= w_1, \\beta = 0.9$\n",
        "\n",
        " so,\n",
        "\n",
        " $m_2 = \\beta m_1 + \\eta \\frac{\\partial c}{\\partial w}|_{w= (w_1-\\beta m_1)}$\n",
        "\n",
        " =>$ m_2 = \\beta  \\eta \\frac{\\partial c}{\\partial w}|_{w=w_0} + \\eta \\frac{\\partial c}{\\partial w}|_{w= (w_1-\\beta m_1)}$\n",
        "\n",
        " => $m_2 = \\eta [\\beta \\frac{\\partial c}{\\partial w}|_{w=w_0} + \\frac{\\partial c}{\\partial w}|_{w= (w_1-\\beta m_1)}]$\n",
        "\n",
        " $\\therefore w_2 = w_1 - m_2$\n",
        "\n",
        "\n",
        "###step 3:-\n",
        "\n",
        "\n",
        " $m_2 = \\eta [\\beta \\frac{\\partial c}{\\partial w}|_{w=w_0} + \\frac{\\partial c}{\\partial w}|_{w= (w_1-\\beta m_1)}], w= w_2, \\beta = 0.9$\n",
        "\n",
        " so,\n",
        "\n",
        " $m_2 = \\beta m_2 + \\eta \\frac{\\partial c}{\\partial w}|_{w= (w_2-\\beta m_2)}$\n",
        "\n",
        " => $m_2 = \\eta [\\beta^2 \\frac{\\partial c}{\\partial w}|_{w=w_0} +\\beta \\frac{\\partial c}{\\partial w}|_{w= (w_1-\\beta m_1)} + \\beta m_2 + \\eta \\frac{\\partial c}{\\partial w}|_{w= (w_2-\\beta m_2)}]$\n",
        "\n",
        " $\\therefore w_3 = w_2 - m_3$\n",
        "\n",
        "\n",
        " ## Advantages:\n",
        " - It is faster than momentum\n",
        " - less oscillation\n",
        " - It reaches close to local or global minima\n",
        "\n",
        "## Disadvantages:\n",
        " - Extra term $\\beta $ although $beta = 0.9$ works fine\n",
        "\n",
        "#Keras Implementation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fwec-reMory"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.optimizers.SGD(learning_rate=0.01, momentum= 0.9, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXiV0vaFLNMI"
      },
      "source": [
        "# Adagrad (Adaptive Gradient):\n",
        "Elongated Bowl problem adagrad can solve.\n",
        "\n",
        "   <img src=\"https://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[8].png\" width=\"600\"\n",
        "     height=\"300\">\n",
        "\n",
        "It try to take very mini or baby step to avoid this zic zac issue problem.\n",
        "\n",
        "### Algorithm:\n",
        "\n",
        "$S\\leftarrow S + \\bigtriangledown _ \\theta j(\\theta) \\otimes  \\bigtriangledown _ \\theta j(\\theta)$\n",
        "\n",
        "$\\theta \\leftarrow \\theta + \\eta \\bigtriangledown _ \\theta j(\\theta) \\phi \\sqrt{S+\\in }$\n",
        "\n",
        "or,\n",
        "\n",
        "$S = S + (\\frac{\\partial c}{\\partial w} |_{w=w})^2$\n",
        "\n",
        "$w = w - \\eta \\frac{\\frac{\\partial c}{\\partial w}}{\\sqrt{S+\\epsilon }}$\n",
        "\n",
        "\n",
        "here,\n",
        "\n",
        "$\\phi = $Elements wise division\n",
        "\n",
        "$\\epsilon = $ To avoid 0 division error $\\epsilon = 10^{-7}$\n",
        "\n",
        "$S = $ Scaling factor\n",
        "\n",
        "$\\otimes = $ Elements wise multiplication\n",
        "\n",
        "\n",
        "###Step 1:\n",
        "\n",
        "$S= 0$, $w= w_0$\n",
        "\n",
        "$S_1 = S_0 + (\\frac{\\partial c}{\\partial w} |_{w=w_0})^2$\n",
        "\n",
        "> $=(\\frac{\\partial c}{\\partial w} |_{w=w_0})^2$\n",
        "\n",
        "$w_1 = w_0 - \\eta \\frac{\\frac{\\partial c}{\\partial w} |_{w=w_0}}{\\sqrt{S_1 + \\epsilon }}$\n",
        "\n",
        "$\\therefore w_1 = w_0 - \\eta \\frac{\\frac{\\partial c}{\\partial w} |_{w=w_0}}{(\\sqrt{\\frac{\\partial c}{\\partial w} |_{w=w_0})^2 + \\epsilon }}$\n",
        "\n",
        "\n",
        "###step 2:\n",
        "\n",
        "$S_1 = (\\frac{\\partial c}{\\partial w} |_{w=w_0})^2$, $w=w_1$\n",
        "\n",
        "$S_2 = S_1 + (\\frac{\\partial c}{\\partial w} |_{w=w_1})^2$\n",
        "\n",
        ">$= (\\frac{\\partial c}{\\partial w} |_{w=w_0})^2 + (\\frac{\\partial c}{\\partial w} |_{w=w_1})^2$\n",
        "\n",
        "$w_2 = w_1 - \\eta \\frac{\\frac{\\partial c}{\\partial w} |_{w=w_1}}{\\sqrt{S_2 + \\epsilon }}$  $$\\rightarrow L_2 norm$$\n",
        "\n",
        "\n",
        "It is decaying the learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlsH2tJVWBja"
      },
      "source": [
        "##Observation:\n",
        "It corrects direction by scaling down the gradient vector along with stepest direction.\n",
        "\n",
        "## Advanges:\n",
        " - It corrects the direction initially\n",
        " - less tuning of learning rate\n",
        "\n",
        "##Disadvantages:\n",
        "- Stops early before reaching global minima (sadle point)\n",
        "- Takes longer time to converge due to decaying learning rate\n",
        "\n",
        "## Notes: This is not recommended to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75YX-YJPXTzj"
      },
      "source": [
        "# RMS prop:\n",
        "It was introduced by Geffrey Hinton et al. to solve the early stoping of Adagrad by accumulating gradient from recent iteration by using exponential decay.\n",
        "\n",
        "### Algorithm:\n",
        "\n",
        "\n",
        "$S\\leftarrow \\beta S + (1-\\beta)\\bigtriangledown _ \\theta j(\\theta) \\otimes  \\bigtriangledown _ \\theta j(\\theta)$\n",
        "\n",
        "$\\theta \\leftarrow \\theta + \\eta \\bigtriangledown _ \\theta j(\\theta) \\phi \\sqrt{S+\\in }$\n",
        "\n",
        "or,\n",
        "\n",
        "$S = \\beta S + (1-\\beta)(\\frac{\\partial c}{\\partial w} |_{w=w})^2$\n",
        "\n",
        "$w = w - \\eta \\frac{\\frac{\\partial c}{\\partial w}}{\\sqrt{S+\\epsilon }}$\n",
        "\n",
        "\n",
        "Here, $\\beta = 0.9$ works well\n",
        "\n",
        "####Note:\n",
        "  - Adagrad - decay would be fast\n",
        "  - RMSprop - decay would be slow\n",
        "\n",
        "### Observation:\n",
        "- It is one of the best optimizer before Adam came up\n",
        "- Although $\\beta = 0.9$ but it's an extra parameter to tune\n",
        "- It was able to solve early stoping case of Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnJWRp8AV3-l"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.optimizers.RMSProp(learning_rate=0.01,rho = 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCyz4wD8arPJ"
      },
      "source": [
        "# Adam Optimizer:\n",
        "This is called as Adaptive moment estimation. This is a combine idea of momentum + RMSprop + eponential decay.\n",
        "\n",
        "### Algorithm:\n",
        "\n",
        "1. $m\\leftarrow \\beta_1 S + (1-\\beta_1)\\bigtriangledown _ \\theta j(\\theta) $\n",
        "2. $S\\leftarrow \\beta_2 S + (1-\\beta_2)\\bigtriangledown _ \\theta j(\\theta) \\otimes  \\bigtriangledown _ \\theta j(\\theta)$\n",
        "3. $\\hat{m} \\leftarrow \\frac{m}{1-\\beta_1^t}$\n",
        "4. $\\hat{S} \\leftarrow \\frac{S}{1-\\beta_2^t}$\n",
        "5. $\\theta \\leftarrow \\theta - \\eta \\hat{m} \\phi \\sqrt{\\hat{S}+\\epsilon }$\n",
        "\n",
        "here,\n",
        "\n",
        "$t=$ iteration step\n",
        "\n",
        "$\\hat{m} = $ bias correction\n",
        "\n",
        "$\\hat{S}$ = smoothing operation\n",
        "\n",
        "\n",
        "or,\n",
        "\n",
        "1. $m = \\beta_1m + (1-\\beta_1) \\frac{\\partial c}{\\partial w}|_w$\n",
        "2. $S = \\beta_2m + (1-\\beta_2) (\\frac{\\partial c}{\\partial w}|_w)^2$\n",
        "3. $\\hat{m} = \\frac{m}{1-\\beta_1^t}$\n",
        "4. $\\hat{S} = \\frac{S}{1-\\beta_2^t}$\n",
        "5. $w= w - \\eta \\frac{\\hat{m}}{\\sqrt{\\hat{S}+\\epsilon }}$\n",
        "\n",
        "\n",
        "For more info please refer the paper\n",
        "[Adam](https://arxiv.org/pdf/1412.6980.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx6GL_gugsbZ"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.optimizers.Adam(learning_rate=0.01,beta_1 = 0.9, beta_2 = 0.999)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}