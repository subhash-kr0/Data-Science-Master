{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02a385d",
   "metadata": {
    "id": "d02a385d"
   },
   "source": [
    "# **THEORY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e029d43",
   "metadata": {
    "id": "5e029d43"
   },
   "source": [
    "## Q1. Explain the concept of batch normalization in the context of Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbaf121",
   "metadata": {
    "id": "9dbaf121"
   },
   "source": [
    "### Batch Normalization is a technique used in neural networks to stabilize and accelerate training. It normalizes the activations within mini-batches during training, reducing internal covariate shift. It helps networks converge faster, allows for higher learning rates, acts as regularization, and improves generalization. Batch Normalization is applied to layers and involves normalizing, scaling, and shifting activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a2548",
   "metadata": {
    "id": "824a2548"
   },
   "source": [
    "## Q2. Describe the benefits of using batch normalization during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac054c",
   "metadata": {
    "id": "a2ac054c"
   },
   "source": [
    "### The benefits of using batch normalization during training in neural networks include:\n",
    "\n",
    "1. **Faster Convergence:** Batch normalization stabilizes training, allowing for quicker convergence. Networks reach their desired accuracy in fewer training iterations.\n",
    "\n",
    "2. **Higher Learning Rates:** It enables the use of higher learning rates, which accelerates training without causing instability. This speeds up the optimization process.\n",
    "\n",
    "3. **Reduced Internal Covariate Shift:** Batch normalization mitigates the problem of internal covariate shift by normalizing activations within mini-batches. This results in more stable gradients and faster training.\n",
    "\n",
    "4. **Regularization:** It acts as a form of regularization, reducing the need for dropout or L2 regularization. This helps prevent overfitting.\n",
    "\n",
    "5. **Improved Generalization:** Batch Normalization often leads to models that generalize better to unseen data, resulting in better test performance.\n",
    "\n",
    "6. **Robustness to Initialization:** Networks with batch normalization are less sensitive to weight initialization, making it easier to train deep architectures.\n",
    "\n",
    "7. **Compatibility with Various Architectures:** Batch Normalization can be used with different types of layers, including fully connected, convolutional, and recurrent layers.\n",
    "\n",
    "8. **Differentiable Operation:** It is designed to be differentiable, allowing gradients to be computed efficiently during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62728098",
   "metadata": {
    "id": "62728098"
   },
   "source": [
    "## Q3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e0cd4d",
   "metadata": {
    "id": "76e0cd4d"
   },
   "source": [
    "### Batch Normalization (BatchNorm) works by normalizing the activations within mini-batches during training to address the problem of internal covariate shift. Here's an overview of its working principle, including the normalization step and the learnable parameters:\n",
    "\n",
    "**Normalization Step:**\n",
    "1. **Mini-Batch Statistics:** For each mini-batch during training, BatchNorm calculates two statistics: the mean μ and variance (σ^2) of the activations across the mini-batch. These statistics provide an estimate of the distribution of activations for that batch.\n",
    "\n",
    "2. **Normalization:** The activations within the mini-batch are then normalized using the calculated mean and variance. This is done element-wise for each activation x within the mini-batch:\n",
    "\n",
    "   x' =  (x - μ) / √(σ^2 + ε)\n",
    "\n",
    "   Here, x' is the normalized activation, μ is the mean, σ^2 is the variance, and ε is a small constant (e.g., 1e-5) added to the denominator for numerical stability.\n",
    "\n",
    "3. **Scaling and Shifting:** After normalization, the activations are scaled by a learnable parameter γ and shifted by another learnable parameter β:\n",
    "\n",
    "   y = \tγ x' + β\n",
    "\n",
    "   The \tγ parameter allows the network to adjust the scale of the normalized activations, and the β parameter allows it to adjust the shift. These parameters are learned during training.\n",
    "\n",
    "#### Learnable Parameters:\n",
    "\n",
    "The key components of BatchNorm are the γ and β parameters:\n",
    "\n",
    "- **γ (Scale Parameter):** It allows the network to control the scale or magnitude of the normalized activations. If \tγ is close to 1, it preserves the distribution learned during normalization. If γ is less than 1, it scales down the activations, and if it's greater than 1, it scales them up.\n",
    "\n",
    "- **β (Shift Parameter):** It allows the network to control the shift or translation of the normalized activations. It can shift the activations away from the standard normal distribution achieved through normalization.\n",
    "\n",
    "\n",
    "During training, the mean and variance are calculated for each mini-batch. However, during inference or when making predictions on a single example, the statistics used for normalization may be calculated differently. Typically, a moving average of the statistics from all mini-batches seen during training is used for normalization during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_UQ3kIyS28lC",
   "metadata": {
    "id": "_UQ3kIyS28lC"
   },
   "source": [
    "## Q4. Discuss the Advantages and Disadvantages of Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WcSEZGIO3Ijh",
   "metadata": {
    "id": "WcSEZGIO3Ijh"
   },
   "source": [
    "**Advantages of Batch Normalization:**\n",
    "\n",
    "1. **Faster Convergence:** BatchNorm stabilizes training by reducing internal covariate shift. Networks with BatchNorm tend to converge faster, requiring fewer training iterations to reach the desired performance.\n",
    "\n",
    "2. **Higher Learning Rates:** It allows the use of higher learning rates without causing instability during training. This can significantly accelerate the optimization process.\n",
    "\n",
    "3. **Regularization:** BatchNorm acts as a form of regularization. By reducing the need for dropout or L2 regularization, it helps prevent overfitting, leading to more generalizable models.\n",
    "\n",
    "4. **Improved Generalization:** Models trained with BatchNorm often generalize better to unseen data. The regularization effect and reduced overfitting contribute to improved test performance.\n",
    "\n",
    "5. **Stability with Deep Networks:** BatchNorm addresses issues like vanishing and exploding gradients in deep networks, making it easier to train very deep architectures.\n",
    "\n",
    "6. **Compatibility with Various Architectures:** It can be applied to different types of layers, including fully connected, convolutional, and recurrent layers, making it a versatile technique.\n",
    "\n",
    "7. **Differentiable Operation:** BatchNorm is designed to be differentiable, enabling efficient computation of gradients during backpropagation.\n",
    "\n",
    "**Disadvantages of Batch Normalization:**\n",
    "\n",
    "1. **Training and Inference Discrepancy:** The statistics (mean and variance) used for normalization during inference may differ from those during training. This can introduce discrepancies between training and inference and impact model performance.\n",
    "\n",
    "2. **Batch Size Sensitivity:** BatchNorm's effectiveness can depend on the batch size. Extremely small batch sizes may lead to less accurate estimates of mean and variance, affecting performance.\n",
    "\n",
    "3. **Impact on Small Networks:** In small networks, BatchNorm may not always provide significant benefits and can even lead to slower training due to the overhead of additional parameters.\n",
    "\n",
    "4. **Added Complexity:** BatchNorm introduces additional parameters (\\(\\gamma\\) and \\(\\beta\\)) that need to be learned during training. This increases the model's complexity and memory requirements.\n",
    "\n",
    "5. **Dependency on Initialization:** The performance of BatchNorm can be sensitive to the choice of initial values for \\(\\gamma\\) and \\(\\beta\\). Poor initialization can lead to slower convergence.\n",
    "\n",
    "6. **Not Always Needed:** In some cases, simpler techniques like weight initialization, careful learning rate schedules, or alternative normalization methods like Layer Normalization or Group Normalization may suffice without the added complexity of BatchNorm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X2Xllqbxy94b",
   "metadata": {
    "id": "X2Xllqbxy94b"
   },
   "source": [
    "# **IMPLEMENTATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IKeMiEy2zEOa",
   "metadata": {
    "id": "IKeMiEy2zEOa"
   },
   "source": [
    "### ***Before Batch Normalization***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ozxji6PBssdq",
   "metadata": {
    "id": "Ozxji6PBssdq"
   },
   "source": [
    "Dataset Description\n",
    "https://keras.io/api/datasets/fashion_mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c4fd50",
   "metadata": {
    "id": "f5c4fd50"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from keras.datasets import fashion_mnist\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wbei8W9Ix21w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbei8W9Ix21w",
    "outputId": "a8785664-2ee2-4c59-8bbe-eecd2c3f28da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-16ea5becd632>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    print('Running on GPU')\n",
    "else:\n",
    "    print('Running on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "W7-XTSJ6sF9q",
   "metadata": {
    "id": "W7-XTSJ6sF9q"
   },
   "outputs": [],
   "source": [
    "#Loading the FashionMnist Dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0 #Typecasting to float\n",
    "X_test = X_test / 255.0 #Typecasting to float\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "FqlXQeSLt-NQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqlXQeSLt-NQ",
    "outputId": "ea5b1ffd-ef12-4583-ed4e-0dace9f0ada3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape , y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "zpsZSy38uASQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpsZSy38uASQ",
    "outputId": "a1ac4857-0228-4457-fcfb-40b7962ee245"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 28, 28), (55000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Bd9HAwoBuFUE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bd9HAwoBuFUE",
    "outputId": "fad75fac-14b3-446f-ba56-4ec6500aa419"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "OI0K_7ojuKRB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OI0K_7ojuKRB",
    "outputId": "43d0b8ac-9b64-4af8-899c-c60da31af748"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 28, 28), (5000, 28, 28))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape , X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "kRVAM3Sdvosk",
   "metadata": {
    "id": "kRVAM3Sdvosk"
   },
   "outputs": [],
   "source": [
    "# Creating layer of model\n",
    "\n",
    "#Setting seed for code reproducability\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "LAYERS = [ tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")]\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "iFQyvWvawUsZ",
   "metadata": {
    "id": "iFQyvWvawUsZ"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0hPTzvyywgIs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hPTzvyywgIs",
    "outputId": "fe8ba8a9-b7df-4b39-93f8-9862f24ef04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266610 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "z_KeAFoVwje_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_KeAFoVwje_",
    "outputId": "5973a8bd-09d9-4976-c54b-3d1bc7eefb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1719/1719 - 10s - loss: 1.2886 - accuracy: 0.6163 - val_loss: 0.8669 - val_accuracy: 0.7348 - 10s/epoch - 6ms/step\n",
      "Epoch 2/15\n",
      "1719/1719 - 8s - loss: 0.7798 - accuracy: 0.7528 - val_loss: 0.6965 - val_accuracy: 0.7796 - 8s/epoch - 5ms/step\n",
      "Epoch 3/15\n",
      "1719/1719 - 6s - loss: 0.6692 - accuracy: 0.7806 - val_loss: 0.6340 - val_accuracy: 0.7962 - 6s/epoch - 4ms/step\n",
      "Epoch 4/15\n",
      "1719/1719 - 5s - loss: 0.6127 - accuracy: 0.7969 - val_loss: 0.5827 - val_accuracy: 0.8144 - 5s/epoch - 3ms/step\n",
      "Epoch 5/15\n",
      "1719/1719 - 5s - loss: 0.5769 - accuracy: 0.8086 - val_loss: 0.5532 - val_accuracy: 0.8230 - 5s/epoch - 3ms/step\n",
      "Epoch 6/15\n",
      "1719/1719 - 5s - loss: 0.5507 - accuracy: 0.8151 - val_loss: 0.5321 - val_accuracy: 0.8302 - 5s/epoch - 3ms/step\n",
      "Epoch 7/15\n",
      "1719/1719 - 4s - loss: 0.5308 - accuracy: 0.8220 - val_loss: 0.5143 - val_accuracy: 0.8330 - 4s/epoch - 3ms/step\n",
      "Epoch 8/15\n",
      "1719/1719 - 5s - loss: 0.5152 - accuracy: 0.8255 - val_loss: 0.5091 - val_accuracy: 0.8300 - 5s/epoch - 3ms/step\n",
      "Epoch 9/15\n",
      "1719/1719 - 5s - loss: 0.5027 - accuracy: 0.8300 - val_loss: 0.4910 - val_accuracy: 0.8346 - 5s/epoch - 3ms/step\n",
      "Epoch 10/15\n",
      "1719/1719 - 4s - loss: 0.4918 - accuracy: 0.8326 - val_loss: 0.4848 - val_accuracy: 0.8378 - 4s/epoch - 3ms/step\n",
      "Epoch 11/15\n",
      "1719/1719 - 5s - loss: 0.4824 - accuracy: 0.8353 - val_loss: 0.4754 - val_accuracy: 0.8398 - 5s/epoch - 3ms/step\n",
      "Epoch 12/15\n",
      "1719/1719 - 4s - loss: 0.4746 - accuracy: 0.8379 - val_loss: 0.4708 - val_accuracy: 0.8404 - 4s/epoch - 3ms/step\n",
      "Epoch 13/15\n",
      "1719/1719 - 5s - loss: 0.4678 - accuracy: 0.8395 - val_loss: 0.4645 - val_accuracy: 0.8426 - 5s/epoch - 3ms/step\n",
      "Epoch 14/15\n",
      "1719/1719 - 5s - loss: 0.4615 - accuracy: 0.8416 - val_loss: 0.4606 - val_accuracy: 0.8426 - 5s/epoch - 3ms/step\n",
      "Epoch 15/15\n",
      "1719/1719 - 5s - loss: 0.4560 - accuracy: 0.8437 - val_loss: 0.4528 - val_accuracy: 0.8472 - 5s/epoch - 3ms/step\n",
      "Runtime of the program is 82.93865251541138\n"
     ]
    }
   ],
   "source": [
    "#Training and Calculating the training time\n",
    "\n",
    "#Starting time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    validation_data=(X_valid,y_valid),\n",
    "                    verbose = 2\n",
    "                    )\n",
    "\n",
    "#Ending time\n",
    "end = time.time()\n",
    "\n",
    "#Total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mdSt7Zl3yTO_",
   "metadata": {
    "id": "mdSt7Zl3yTO_"
   },
   "source": [
    "### Observation:\n",
    "- Runtime of the program is 82.9 sec\n",
    "- Accuracy: 0.8437"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E_I06pl7zRlp",
   "metadata": {
    "id": "E_I06pl7zRlp"
   },
   "source": [
    "### ***After Batch Normalization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dya2M7b-zS3f",
   "metadata": {
    "id": "dya2M7b-zS3f"
   },
   "outputs": [],
   "source": [
    "# delete the previous model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "yb1_KXPezVsl",
   "metadata": {
    "id": "yb1_KXPezVsl"
   },
   "outputs": [],
   "source": [
    "# Defing new model with batch normalization\n",
    "\n",
    "tf.random.set_seed(42)#Setting seed for code reproducability\n",
    "np.random.seed(42)\n",
    "\n",
    "LAYERS_BN = [\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "]\n",
    "\n",
    "model = tf.keras.models.Sequential(LAYERS_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "AMt5FidTzjDL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMt5FidTzjDL",
    "outputId": "2652c803-a832-4117-a72f-ff54339b259d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 784)               3136      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Wsn28ku0ztFr",
   "metadata": {
    "id": "Wsn28ku0ztFr"
   },
   "outputs": [],
   "source": [
    "bn1 = model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "EZZbpGZDzxXO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZZbpGZDzxXO",
    "outputId": "e4164469-6af6-4fdc-f28a-82ea89af354b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization/gamma:0 True\n",
      "batch_normalization/beta:0 True\n",
      "batch_normalization/moving_mean:0 False\n",
      "batch_normalization/moving_variance:0 False\n"
     ]
    }
   ],
   "source": [
    "for variable in bn1.variables:\n",
    "  print(variable.name, variable.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "SCB0OJ3NzdjZ",
   "metadata": {
    "id": "SCB0OJ3NzdjZ"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8RcMta9ezf50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RcMta9ezf50",
    "outputId": "e0aeee78-665e-4e49-c726-2d082f10091a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1719/1719 - 9s - loss: 0.8363 - accuracy: 0.7165 - val_loss: 0.5422 - val_accuracy: 0.8170 - 9s/epoch - 5ms/step\n",
      "Epoch 2/15\n",
      "1719/1719 - 8s - loss: 0.5643 - accuracy: 0.8033 - val_loss: 0.4678 - val_accuracy: 0.8418 - 8s/epoch - 5ms/step\n",
      "Epoch 3/15\n",
      "1719/1719 - 7s - loss: 0.5113 - accuracy: 0.8206 - val_loss: 0.4320 - val_accuracy: 0.8528 - 7s/epoch - 4ms/step\n",
      "Epoch 4/15\n",
      "1719/1719 - 8s - loss: 0.4733 - accuracy: 0.8342 - val_loss: 0.4126 - val_accuracy: 0.8550 - 8s/epoch - 5ms/step\n",
      "Epoch 5/15\n",
      "1719/1719 - 9s - loss: 0.4482 - accuracy: 0.8416 - val_loss: 0.3976 - val_accuracy: 0.8602 - 9s/epoch - 5ms/step\n",
      "Epoch 6/15\n",
      "1719/1719 - 7s - loss: 0.4317 - accuracy: 0.8467 - val_loss: 0.3884 - val_accuracy: 0.8626 - 7s/epoch - 4ms/step\n",
      "Epoch 7/15\n",
      "1719/1719 - 8s - loss: 0.4209 - accuracy: 0.8510 - val_loss: 0.3765 - val_accuracy: 0.8654 - 8s/epoch - 4ms/step\n",
      "Epoch 8/15\n",
      "1719/1719 - 8s - loss: 0.4073 - accuracy: 0.8549 - val_loss: 0.3727 - val_accuracy: 0.8664 - 8s/epoch - 5ms/step\n",
      "Epoch 9/15\n",
      "1719/1719 - 7s - loss: 0.3989 - accuracy: 0.8590 - val_loss: 0.3671 - val_accuracy: 0.8682 - 7s/epoch - 4ms/step\n",
      "Epoch 10/15\n",
      "1719/1719 - 8s - loss: 0.3874 - accuracy: 0.8628 - val_loss: 0.3619 - val_accuracy: 0.8700 - 8s/epoch - 4ms/step\n",
      "Epoch 11/15\n",
      "1719/1719 - 8s - loss: 0.3786 - accuracy: 0.8652 - val_loss: 0.3566 - val_accuracy: 0.8708 - 8s/epoch - 4ms/step\n",
      "Epoch 12/15\n",
      "1719/1719 - 8s - loss: 0.3761 - accuracy: 0.8658 - val_loss: 0.3550 - val_accuracy: 0.8710 - 8s/epoch - 4ms/step\n",
      "Epoch 13/15\n",
      "1719/1719 - 8s - loss: 0.3675 - accuracy: 0.8677 - val_loss: 0.3510 - val_accuracy: 0.8738 - 8s/epoch - 4ms/step\n",
      "Epoch 14/15\n",
      "1719/1719 - 7s - loss: 0.3595 - accuracy: 0.8708 - val_loss: 0.3491 - val_accuracy: 0.8734 - 7s/epoch - 4ms/step\n",
      "Epoch 15/15\n",
      "1719/1719 - 8s - loss: 0.3535 - accuracy: 0.8753 - val_loss: 0.3449 - val_accuracy: 0.8748 - 8s/epoch - 5ms/step\n",
      "Runtime of the program is 116.63962602615356\n"
     ]
    }
   ],
   "source": [
    "#Training and Calculating the training time\n",
    "\n",
    "#Starting time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    validation_data=(X_valid,y_valid),\n",
    "                    verbose = 2\n",
    "                    )\n",
    "\n",
    "#Ending time\n",
    "end = time.time()\n",
    "\n",
    "#Total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5BzO8YOz1_hv",
   "metadata": {
    "id": "5BzO8YOz1_hv"
   },
   "source": [
    "### Observation:\n",
    "- Runtime of the program is 121.23 sec\n",
    "- accuracy: 0.8741"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BPx4Vtq32WLy",
   "metadata": {
    "id": "BPx4Vtq32WLy"
   },
   "source": [
    "# Conclusion:\n",
    "\n",
    "## Before Applying Batch Normalization\n",
    "- Runtime of the program is 82.9 sec\n",
    "- Accuracy: 0.8437\n",
    "\n",
    "## After Applying Batch Normalization\n",
    "- Runtime of the program is 121.23 sec\n",
    "- accuracy: 0.8741"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
