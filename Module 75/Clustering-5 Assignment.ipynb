{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Assignment pdf link](./01%20May_AssQ.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 1</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A contingency matrix, also known as a confusion matrix, is a table that is used to evaluate the performance of a classification model. It is a square matrix that displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by a binary classifier.\n",
    "\n",
    "### Here's an example of a contingency matrix for a binary classification problem:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Binary Classification](https://miro.medium.com/v2/resize:fit:0/1*3yGLac6F4mTENnj5dBNvNQ.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this example, TP represents the number of true positive predictions (the model predicted positive and the actual value was positive), TN represents the number of true negative predictions (the model predicted negative and the actual value was negative), FP represents the number of false positive predictions (the model predicted positive but the actual value was negative), and FN represents the number of false negative predictions (the model predicted negative but the actual value was positive).\n",
    "\n",
    "### The contingency matrix can be used to calculate various metrics for evaluating the performance of a classification model, including accuracy, precision, recall, and F1 score. These metrics provide different insights into the performance of the model and can be used to compare different models or to optimize the parameters of a given model.\n",
    "\n",
    "### For example, accuracy is a measure of the overall correctness of the model and is calculated as (TP + TN) / (TP + TN + FP + FN). Precision measures the proportion of positive predictions that were correct and is calculated as TP / (TP + FP). Recall measures the proportion of actual positives that were correctly identified and is calculated as TP / (TP + FN). The F1 score is the harmonic mean of precision and recall and is calculated as 2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "### By examining the values in the contingency matrix and calculating these performance metrics, you can gain insights into the strengths and weaknesses of a classification model and make improvements to enhance its accuracy and effectiveness."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 2</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 : How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A pair confusion matrix is a type of confusion matrix that is used to evaluate the performance of a multi-class classification model. In contrast to a regular confusion matrix that is used for binary classification, a pair confusion matrix considers the confusion between pairs of classes.\n",
    "\n",
    "### A pair confusion matrix is constructed by considering each pair of classes and counting the number of instances that are correctly classified and incorrectly classified. Here's an example of a pair confusion matrix for a multi-class classification problem with four classes "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pairwise Confusion](./pairwise%20Confusion.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this example, the rows represent the true labels, and the columns represent the predicted labels. The (i, j) element in the matrix represents the number of instances that belong to class i but were classified as class j.\n",
    "\n",
    "### A pair confusion matrix is useful in situations where you want to evaluate the performance of a multi-class classification model, particularly when the classes are imbalanced. By considering the confusion between pairs of classes, you can gain insights into which classes are frequently confused with each other and which ones are easier to distinguish. This information can help you make improvements to the model, such as adjusting the classification threshold, re-balancing the classes, or improving the feature set.\n",
    "\n",
    "### In addition to the pair confusion matrix, there are other types of confusion matrices that can be used to evaluate the performance of multi-class classification models, such as the micro-average confusion matrix and the macro-average confusion matrix. These matrices provide different ways of aggregating the results across classes and can help you gain a more nuanced understanding of the model's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 3</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the context of natural language processing (NLP), an extrinsic measure is a way of evaluating the performance of a language model by measuring how well it performs on a downstream task that relies on language understanding.\n",
    "\n",
    "### Extrinsic measures are often used to evaluate the performance of language models because they provide a more realistic assessment of the model's ability to understand and generate natural language. Unlike intrinsic measures, which evaluate the model's performance on a specific task or metric, extrinsic measures evaluate the model's performance on a broader range of tasks that are relevant to real-world applications.\n",
    "\n",
    "### For example, an extrinsic measure in NLP might be to evaluate a language model's ability to accurately classify news articles as positive or negative sentiment. In this case, the language model would be trained on a large corpus of text that includes examples of both positive and negative sentiment, and its performance would be evaluated on how well it classifies new, unseen text.\n",
    "\n",
    "### Extrinsic measures can be used to evaluate a variety of different types of language models, including neural networks, statistical models, and rule-based systems. They are typically evaluated using standard benchmarks and datasets that are designed to test the model's performance on a specific task.\n",
    "\n",
    "### Overall, extrinsic measures provide a valuable way of evaluating the performance of language models in real-world scenarios, and they are an important tool for researchers and developers working in the field of natural language processing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 4</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the context of machine learning, an intrinsic measure is a way of evaluating the performance of a model based on its performance on a specific task or metric, without considering how well it performs on downstream tasks. In contrast, an extrinsic measure evaluates the performance of a model on a downstream task that depends on the model's output as an input.\n",
    "\n",
    "### An intrinsic measure is often used during the development and training phases of a model to assess its performance and to make adjustments to the model's architecture or hyperparameters. Intrinsic measures can be used to evaluate various aspects of a model's performance, such as accuracy, precision, recall, F1 score, and others. For example, in the context of image classification, an intrinsic measure might be to evaluate the accuracy of a model in correctly classifying images based on their content.\n",
    "\n",
    "### Extrinsic measures, on the other hand, evaluate the performance of a model on a specific downstream task that is relevant to a particular application or domain. This type of evaluation is typically performed after the model has been trained and deployed, and it is used to assess how well the model performs in real-world scenarios. For example, in the context of natural language processing, an extrinsic measure might be to evaluate the performance of a language model on sentiment analysis or named entity recognition tasks.\n",
    "\n",
    "### The main difference between intrinsic and extrinsic measures is that intrinsic measures evaluate the performance of a model based on its performance on a specific task or metric, while extrinsic measures evaluate the performance of a model on a downstream task that depends on its output as an input. Intrinsic measures are useful during the development and training phases of a model, while extrinsic measures are used to evaluate the model's performance in real-world scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 5</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 : What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A confusion matrix is a table that is used to evaluate the performance of a classification model by comparing the predicted labels with the true labels. The main purpose of a confusion matrix is to help assess the quality of the model's predictions and identify areas where the model may need improvement.\n",
    "\n",
    "### In a confusion matrix, the rows represent the true labels, and the columns represent the predicted labels. Each cell in the matrix represents the number of instances that were classified into a particular combination of true and predicted labels. The confusion matrix can be used to compute various performance metrics, such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "### By analyzing the confusion matrix, you can identify strengths and weaknesses of a model. For example:\n",
    "\n",
    "1. `True positives (TP)`: Instances that were correctly classified as positive. A high number of true positives indicates that the model is performing well in predicting positive instances.\n",
    "\n",
    "2. `False positives (FP)`: Instances that were incorrectly classified as positive. A high number of false positives indicates that the model is prone to making false positive predictions.\n",
    "\n",
    "3. `True negatives (TN)`: Instances that were correctly classified as negative. A high number of true negatives indicates that the model is performing well in predicting negative instances.\n",
    "\n",
    "4. `False negatives (FN)`: Instances that were incorrectly classified as negative. A high number of false negatives indicates that the model is prone to making false negative predictions.\n",
    "\n",
    "### By analyzing the distribution of these metrics across the confusion matrix, you can identify specific areas where the model may need improvement. For example, if the model is prone to making false positive predictions, you may need to adjust the model's threshold or re-balance the training data. If the model is prone to making false negative predictions, you may need to collect more training data or adjust the model's parameters.\n",
    "\n",
    "### Overall, a confusion matrix is a powerful tool for evaluating the performance of a classification model and identifying areas where the model can be improved. It provides a clear and concise summary of the model's predictions, and it can be used to guide the development and refinement of the model over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 6</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 : What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning algorithms are typically evaluated based on their ability to discover patterns or structure in unlabeled data. Unlike supervised learning, there are no known labels to evaluate the quality of unsupervised learning results. Therefore, intrinsic measures are used to evaluate the performance of unsupervised learning algorithms based on the internal quality of the clusters or patterns discovered in the data. Here are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms:\n",
    "\n",
    "1. `Silhouette score`: The silhouette score measures how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, where a score closer to 1 indicates that an object is well-matched to its own cluster and poorly-matched to neighboring clusters.\n",
    "\n",
    "2. `Davies-Bouldin Index`: The Davies-Bouldin Index measures the average similarity between each cluster and its most similar cluster, normalized by the average distance between clusters. Lower values of this index indicate better clustering.\n",
    "\n",
    "3. `Calinski-Harabasz Index`: The Calinski-Harabasz Index measures the ratio of between-cluster variance to within-cluster variance. Higher values of this index indicate better clustering.\n",
    "\n",
    "4. `Dunn Index`: The Dunn Index measures the ratio of the distance between the nearest objects from different clusters to the distance between the farthest objects in the same cluster. Higher values of this index indicate better clustering.\n",
    "\n",
    "### These intrinsic measures can be used to compare different unsupervised learning algorithms or to tune the parameters of a single algorithm. However, it is important to keep in mind that these measures do not provide information on the usefulness of the discovered patterns or clusters for downstream tasks. Therefore, it is recommended to use extrinsic measures to evaluate the performance of unsupervised learning algorithms in real-world scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 7</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 : What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While accuracy is a commonly used evaluation metric for classification tasks, there are some limitations to using it as the sole evaluation metric. Here are some of the limitations:\n",
    "\n",
    "1. `Imbalanced classes`: When the distribution of classes in the dataset is imbalanced, accuracy can be a misleading metric. For example, if 95% of the samples in the dataset belong to one class, a model that always predicts that class will have 95% accuracy. However, this model may not be useful in practice. To address this limitation, alternative metrics like precision, recall, F1 score, and area under the ROC curve (AUC-ROC) can be used.\n",
    "\n",
    "2. `Cost-sensitive classification`: In some scenarios, the cost of misclassifying a sample from one class can be significantly higher than misclassifying a sample from another class. In these cases, accuracy alone may not be a sufficient metric. Instead, cost-sensitive evaluation metrics like weighted accuracy or weighted F1 score can be used.\n",
    "\n",
    "3. `Multiclass classification`: In multiclass classification, accuracy may not provide enough information about the model's performance. For example, a model that performs well for one class but poorly for another may still have high accuracy. In this case, metrics like macro-averaged F1 score, micro-averaged F1 score, and confusion matrix can provide more detailed information about the model's performance.\n",
    "\n",
    "### To address these limitations, it is recommended to use multiple evaluation metrics that complement each other. For example, using both accuracy and F1 score can provide a more comprehensive evaluation of the model's performance. Additionally, it is important to consider the specific characteristics of the dataset and the problem at hand when selecting the evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
